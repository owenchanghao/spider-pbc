{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People's Bank of China: a Web Scrapper\n",
    "\n",
    "Changhao Li | 2021.12\n",
    "\n",
    "\n",
    "``GOAL`` of this programme: extract data, especially fines issued towards payment companies. Due to the structure obsticles, pure-automatically extracting those information is difficult.\n",
    "Therefore, we adopt a semi-structured method towards our goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Limitation of extracting data from a PDF format file\n",
    "\n",
    "It's difficult to read data, especially those written in Chinese character, from a PDF file. For instance, we try to extract relevant information from a sample PDF by **camelot**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TableList n=1>\n",
      "[['备注']]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: No tables found in table area 1 [stream.py:365]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from camelot import read_pdf\n",
    "import re\n",
    "\n",
    "def parse_pdf_camelot(link):\n",
    "    tables = read_pdf(link, pages = '0', flavor = 'stream', table_area = ['']) # stream会默认整页均为表格\n",
    "    print(tables)\n",
    "    print(tables[0].data)\n",
    "    print()\n",
    "    #print(re.findall(r'\\w+\\s元|\\w+\\s万元', info))\n",
    "    \n",
    "parse_pdf_camelot('2021122718272373606.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot work...\n",
    "\n",
    "Let me try another way to read-in pdf file: use **PDFMiner3K** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program is running...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### PDFMiner reading online PDF files\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "from io import open\n",
    "\n",
    "def readPDF(pdfFile):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams = laparams)\n",
    "    \n",
    "    PDFPage.get_pages(rsrcmgr, device, pdfFile)\n",
    "    device.close()\n",
    "    \n",
    "    content = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return content\n",
    "\n",
    "pdfFile = urlopen('http://nanning.pbc.gov.cn/nanning/133346/133364/133371/4432873/2021122718272373606.pdf')\n",
    "outputString = readPDF(pdfFile)\n",
    "print('The program is running...')\n",
    "print(outputString)\n",
    "pdfFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing happened... Now let's look at reading local files, and see whether it works or not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program is running...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### PDFMiner reading downloaded PDF files\n",
    "\n",
    "#from urllib.request import urlopen\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "from io import open\n",
    "\n",
    "def readPDF(pdfFile):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams = laparams)\n",
    "    \n",
    "    PDFPage.get_pages(rsrcmgr, device, pdfFile)\n",
    "    device.close()\n",
    "    \n",
    "    content = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return content\n",
    "\n",
    "outputString = readPDF('2021122718272373606.pdf')\n",
    "print('The program is running...')\n",
    "print(outputString)\n",
    "pdfFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope. Nothing happened. Hopelessness.\n",
    "\n",
    "The solution: skip the difficult part. This us not an PhD project. Waiting for Dr Josiah Poon, Dr Caren Han and their USYD NLP Group's breakthrough. Bless!\n",
    "\n",
    "In the main part I will only focus on extracting fine url from different PBC branch website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PBC... so many branches!\n",
    "\n",
    "There are 35 PBC websites... 35! Each with different websites! The structure of the websites are different, the format of the information is also different... This makes web scraping extremely complex.\n",
    "\n",
    "For instance, PBC Shenzhen branch issue fine information in Excel format (.xls), PBC Xi'an branch use Word (.doc) instead, PBC Nanning branch use PDF files (.pdf), and PBC Nanjing branch issues pure words (HTML)! What a diversity...\n",
    "\n",
    "全国央行分支机构各自拥有其独立的网站【网站结构不同】、各个网站也单独发布罚单【格式不同】，这让爬虫变得异常复杂。\n",
    "\n",
    "不同央行分行的行政处罚罚单格式不尽相同————例如，深圳市中心支行的罚单信息为Excel格式（.xls）、西安分行公布的附件为Word格式（.doc）、南宁中心支行的则为PDF格式（.pdf）；更有甚者，南京分行的罚单信息竟然为网页纯文字......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Guangzhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 00:00:00\n",
      "2021-12-27 00:00:00\n",
      "2021-12-24 00:00:00\n",
      "2021-12-23 00:00:00\n",
      "http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4433798/index.html\n",
      "http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4433789/index.html\n",
      "http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4433778/index.html\n",
      "http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4433773/index.html\n",
      "['2021-12-27', 'http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4433798/index.html']\n",
      "['2021-12-27', 'http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4433789/index.html']\n",
      "['2021-12-24', 'http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4433778/index.html']\n",
      "['2021-12-23', 'http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4433773/index.html']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding = utf-8 -*-\n",
    "\n",
    "### GUANGZHOU BRANCH\n",
    "'''\n",
    "INPUT: \n",
    "- name: name of the city\n",
    "- year: year\n",
    "- month: month\n",
    "OUTPUT:\n",
    "- a csv file (e.g. 'guangzhou-2021-12.csv')\n",
    "'''\n",
    "\n",
    "from datetime import datetime, date\n",
    "from urllib import request, parse\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from fake_useragent import UserAgent\n",
    "import csv\n",
    "import re\n",
    "import webbrowser\n",
    "\n",
    "ua = UserAgent()\n",
    "\n",
    "name = 'guangzhou'\n",
    "year = '2021'\n",
    "month = '12'\n",
    "\n",
    "def gzSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    mylist = []\n",
    "    finallist = []\n",
    "    \n",
    "    count = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        # print(item)\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                # print(inner)\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                ### INPUT DESIRED TIME HERE!\n",
    "                if ((d > datetime(2021, 12, 1)) & (d < datetime(2021, 12, 31))):\n",
    "                    print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "            l = temp.select('a[href]', limit=count)\n",
    "            for k in range(0,len(l)):\n",
    "                print(\"http://guangzhou.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://guangzhou.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{n}-{y}-{m}.csv'\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    #webbrowser.open('')\n",
    "\n",
    "### INPUT GZ OFFICIAL WEBSITE INSIDE                \n",
    "gzSpider('http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
