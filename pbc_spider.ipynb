{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Universal Variables Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding = utf-8 -*-\n",
    "\n",
    "from datetime import datetime, date\n",
    "from urllib import request, parse\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from fake_useragent import UserAgent\n",
    "import csv\n",
    "import re\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "### INPUT HERE!\n",
    "year = '2022'\n",
    "month = '1'\n",
    "\n",
    "### AUTO JUDGE DAY\n",
    "if int(month) in {1,3,5,7,8,10,12}:\n",
    "    day = '31'\n",
    "elif int(month) in {4,6,9,11}:\n",
    "    day = '30'\n",
    "else:\n",
    "    if (year % 4) == 0 and (year % 100) != 0 or (year % 400) == 0:\n",
    "        day = '29'\n",
    "    else:\n",
    "        day = '28'\n",
    "print(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regional Branches (9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Shanghai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://shanghai.pbc.gov.cn/fzhshanghai/113577/114832/114918/4442474/index.html\n",
      "http://shanghai.pbc.gov.cn/fzhshanghai/113577/114832/114918/4442469/index.html\n",
      "['2022-01-10', 'http://shanghai.pbc.gov.cn/fzhshanghai/113577/114832/114918/4442474/index.html']\n",
      "['2022-01-10', 'http://shanghai.pbc.gov.cn/fzhshanghai/113577/114832/114918/4442469/index.html']\n"
     ]
    }
   ],
   "source": [
    "name = 'shanghai'\n",
    "\n",
    "def shSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://shanghai.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://shanghai.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better.\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "shSpider('http://shanghai.pbc.gov.cn/fzhshanghai/113577/114832/114918/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Tianjin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a single fine found...\n"
     ]
    }
   ],
   "source": [
    "name = 'tianjin'\n",
    "\n",
    "def tjSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://tianjin.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://tianjin.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better.\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "tjSpider('http://tianjin.pbc.gov.cn/fzhtianjin/113682/113700/113707/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Shenyang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://shenyang.pbc.gov.cn/shenyfh/108074/108127/108208/4446900/index.html\n",
      "['2022-01-13', 'http://shenyang.pbc.gov.cn/shenyfh/108074/108127/108208/4446900/index.html']\n"
     ]
    }
   ],
   "source": [
    "name = 'shenyang'\n",
    "\n",
    "def sySpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://shenyang.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://shenyang.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better.\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "sySpider('http://shenyang.pbc.gov.cn/shenyfh/108074/108127/108208/index.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Nanjing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a single fine found...\n"
     ]
    }
   ],
   "source": [
    "name = 'nanjing'\n",
    "\n",
    "def njSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://nanjing.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://nanjing.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better.\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "njSpider('http://nanjing.pbc.gov.cn/nanjing/117542/117560/117567/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Jinan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://jinan.pbc.gov.cn/jinan/120967/120985/120994/4438577/index.html\n",
      "['2022-01-04', 'http://jinan.pbc.gov.cn/jinan/120967/120985/120994/4438577/index.html']\n"
     ]
    }
   ],
   "source": [
    "name = 'jinan'\n",
    "\n",
    "def jnSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://jinan.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://jinan.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "jnSpider('http://jinan.pbc.gov.cn/jinan/120967/120985/120994/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Wuhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://wuhan.pbc.gov.cn/wuhan/123472/123493/123502/4450995/index.html\n",
      "http://wuhan.pbc.gov.cn/wuhan/123472/123493/123502/4450712/index.html\n",
      "http://wuhan.pbc.gov.cn/wuhan/123472/123493/123502/4444018/index.html\n",
      "http://wuhan.pbc.gov.cn/wuhan/123472/123493/123502/4443985/index.html\n",
      "['2022-01-18', 'http://wuhan.pbc.gov.cn/wuhan/123472/123493/123502/4450995/index.html']\n",
      "['2022-01-18', 'http://wuhan.pbc.gov.cn/wuhan/123472/123493/123502/4450712/index.html']\n",
      "['2022-01-10', 'http://wuhan.pbc.gov.cn/wuhan/123472/123493/123502/4444018/index.html']\n",
      "['2022-01-06', 'http://wuhan.pbc.gov.cn/wuhan/123472/123493/123502/4443985/index.html']\n"
     ]
    }
   ],
   "source": [
    "name = 'wuhan'\n",
    "\n",
    "def whSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://wuhan.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://wuhan.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "whSpider('http://wuhan.pbc.gov.cn/wuhan/123472/123493/123502/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 Guangzhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4441872/index.html\n",
      "http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4446669/index.html\n",
      "['2022-01-07', 'http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4441872/index.html']\n",
      "['2022-01-04', 'http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/4446669/index.html']\n"
     ]
    }
   ],
   "source": [
    "name = 'guangzhou'\n",
    "\n",
    "def gzSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://guangzhou.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://guangzhou.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "gzSpider('http://guangzhou.pbc.gov.cn/guangzhou/129142/129159/129166/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 Chengdu\n",
    "\n",
    "Note Chengdu's column width is 66 (wow, how unique)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://chengdu.pbc.gov.cn/chengdu/129320/129341/129350/4449474/index.html\n",
      "http://chengdu.pbc.gov.cn/chengdu/129320/129341/129350/4445512/index.html\n",
      "http://chengdu.pbc.gov.cn/chengdu/129320/129341/129350/4444217/index.html\n",
      "['2022-01-17', 'http://chengdu.pbc.gov.cn/chengdu/129320/129341/129350/4449474/index.html']\n",
      "['2022-01-11', 'http://chengdu.pbc.gov.cn/chengdu/129320/129341/129350/4445512/index.html']\n",
      "['2022-01-10', 'http://chengdu.pbc.gov.cn/chengdu/129320/129341/129350/4444217/index.html']\n"
     ]
    }
   ],
   "source": [
    "name = 'chengdu'\n",
    "\n",
    "def cdSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "   \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT, Chengdu's width is 66\n",
    "            for inner in temp.find_all(\"td\", width=\"66\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://chengdu.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://chengdu.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "cdSpider('http://chengdu.pbc.gov.cn/chengdu/129320/129341/129350/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 Xi'an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a single fine found...\n"
     ]
    }
   ],
   "source": [
    "name = 'xian'\n",
    "\n",
    "def xaSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://xian.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://xian.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "xaSpider('http://xian.pbc.gov.cn/xian/129428/129449/129458/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Direct Operational Dept. (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Beijing\n",
    "\n",
    "Beijing's PBC official website has been changed to width 140 instead of an usual 100. After a series of tests (prints), I spotted this issue and changed 100 to 140 to fix the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'beijing'\n",
    "\n",
    "def bjSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(3)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    prevd = 0\n",
    "    count = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT, note Beijing has width 140!\n",
    "            for inner in temp.find_all(\"td\", width=\"140\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://beijing.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://beijing.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "bjSpider('http://beijing.pbc.gov.cn/beijing/132030/132052/132059/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Chongqing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need further refinement.\n",
    "\n",
    "name = 'chongqing'\n",
    "\n",
    "def cqSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(3)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"div\", id = \"8000\")\n",
    "    # print(fram.prettify())\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\"):\n",
    "        # print(item)\n",
    "        for temp in item.find_all(\"td\", limit=3):\n",
    "            print(temp)\n",
    "            print(\"end of a loop...\")\n",
    "            ### FILTER ALL TIME OUT       \n",
    "            inner = temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\")\n",
    "            print(inner.text)\n",
    "            '''\n",
    "            d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "            print(d)\n",
    "\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count)\n",
    "            for k in range(0,len(l)):\n",
    "                print(\"http://chongqing.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://chongqing.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "'''\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "cqSpider('http://chongqing.pbc.gov.cn/chongqing/107680/107897/107909/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Capital Branches (20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Shijiazhuang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'shijiazhuang'\n",
    "\n",
    "def sjzSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://shijiazhuang.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://shijiazhuang.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "sjzSpider('http://shijiazhuang.pbc.gov.cn/shijiazhuang/131442/131463/131472/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Taiyuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'taiyuan'\n",
    "\n",
    "def tySpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://taiyuan.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://taiyuan.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "tySpider('http://taiyuan.pbc.gov.cn/taiyuan/133960/133981/133988/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Huhhot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'huhhot'\n",
    "\n",
    "def hhSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://huhehaote.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://huhehaote.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "hhSpider('http://huhehaote.pbc.gov.cn/huhehaote/129797/129815/129822/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Changchun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'changchun'\n",
    "\n",
    "def ccSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://changchun.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://changchun.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "ccSpider('http://changchun.pbc.gov.cn/changchun/124680/124698/124705/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Harbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'harbin'\n",
    "\n",
    "def hbSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://haerbin.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://haerbin.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "hbSpider('http://haerbin.pbc.gov.cn/haerbin/112693/112776/112783/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Hangzhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'hangzhou'\n",
    "\n",
    "def hzSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://hangzhou.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://hangzhou.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "hzSpider('http://hangzhou.pbc.gov.cn/hangzhou/125268/125286/125293/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Fuzhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'fuzhou'\n",
    "\n",
    "def fzSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://fuzhou.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://fuzhou.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "fzSpider('http://fuzhou.pbc.gov.cn/fuzhou/126805/126823/126830/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Hefei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'hefei'\n",
    "\n",
    "def hfSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://hefei.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://hefei.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "hfSpider('http://hefei.pbc.gov.cn/hefei/122364/122382/122389/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 Zhengzhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'zhengzhou'\n",
    "\n",
    "def zzSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://zhengzhou.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://zhengzhou.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "zzSpider('http://zhengzhou.pbc.gov.cn/zhengzhou/124182/124200/124207/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10 Changsha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'changsha'\n",
    "\n",
    "def csSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://changsha.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://changsha.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "csSpider('http://changsha.pbc.gov.cn/changsha/130011/130029/130036/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.11 Nanchang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'nanchang'\n",
    "\n",
    "def ncSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://nanchang.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://nanchang.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "ncSpider('http://nanchang.pbc.gov.cn/nanchang/132372/132390/132397/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.12 Nanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'nanning'\n",
    "\n",
    "def nnnSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://nanning.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://nanning.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "nnnSpider('http://nanning.pbc.gov.cn/nanning/133346/133364/133371/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.13 Haikou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'haikou'\n",
    "\n",
    "def hkSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://haikou.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://haikou.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "hkSpider('http://haikou.pbc.gov.cn/haikou/132982/133000/133007/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.14 Kunming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'kunming'\n",
    "\n",
    "def kmSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://kunming.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://kunming.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "kmSpider('http://kunming.pbc.gov.cn/kunming/133736/133760/133767/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.15 Guiyang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'guiyang'\n",
    "\n",
    "def gySpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://guiyang.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://guiyang.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "gySpider('http://guiyang.pbc.gov.cn/guiyang/113288/113306/113313/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.16 Lhasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lhasa'\n",
    "\n",
    "def lasaSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://lasa.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://lasa.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "lasaSpider('http://lasa.pbc.gov.cn/lasa/120480/120504/120511/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.17 Lanzhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lanzhou'\n",
    "\n",
    "def lzSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://lanzhou.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://lanzhou.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "lzSpider('http://lanzhou.pbc.gov.cn/lanzhou/117067/117091/117098/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.18 Xining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'xining'\n",
    "\n",
    "def xnSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://xining.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://xining.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "xnSpider('http://xining.pbc.gov.cn/xining/118239/118263/118270/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.19 Yinchuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'yinchuan'\n",
    "\n",
    "def ycSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://yinchuan.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://yinchuan.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "ycSpider('http://yinchuan.pbc.gov.cn/yinchuan/119983/120001/120008/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.20 Urumqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'urumqi'\n",
    "\n",
    "def urmqSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://wulumuqi.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://wulumuqi.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "urmqSpider('http://wulumuqi.pbc.gov.cn/wulumuqi/121755/121777/121784/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Non-capital Branches (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Shenzhen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'shenzhen'\n",
    "\n",
    "def szSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://shenzhen.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://shenzhen.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "szSpider('http://shenzhen.pbc.gov.cn/shenzhen/122811/122833/122840/index.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Dalian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'dalian'\n",
    "\n",
    "def dlSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://dalian.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://dalian.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "dlSpider('http://dalian.pbc.gov.cn/dalian/123812/123830/123837/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Qingdao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'qingdao'\n",
    "\n",
    "def qdSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://qingdao.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://qingdao.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "qdSpider('http://qingdao.pbc.gov.cn/qingdao/126166/126184/126191/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Ningbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ningbo'\n",
    "\n",
    "def nbSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://ningbo.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://ningbo.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "nbSpider('http://ningbo.pbc.gov.cn/ningbo/127076/127098/127105/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Xiamen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'xiamen'\n",
    "\n",
    "def xmSpider(link):\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(1)\n",
    "    driver.get(link)\n",
    "    req = driver.page_source\n",
    "    # print(req)\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    # print(soup.prettify())\n",
    "    fram = soup.find(\"td\", class_ = \"content_right column\")\n",
    "    # print(fram.prettify())\n",
    "    \n",
    "    count = 0\n",
    "    prevd = 0\n",
    "    datelist = []\n",
    "    linklist = []\n",
    "    for item in fram.find_all(\"table\", limit=1):\n",
    "        for temp in item.find_all(\"td\", limit=1):\n",
    "            ### FILTER ALL TIME OUT\n",
    "            for inner in temp.find_all(\"td\", width=\"100\", class_=\"hei12jj\", limit=10):\n",
    "                d = datetime.strptime(inner.text, '%Y-%m-%d')\n",
    "                if ((d >= datetime(int(year), int(month), 1)) \n",
    "                    & (d <= datetime(int(year), int(month), int(day)))):\n",
    "                    #print(d)\n",
    "                    datelist.append(d)\n",
    "                    count += 1\n",
    "                elif (d > datetime(int(year), int(month), int(day))):\n",
    "                    prevd += 1\n",
    "                else:\n",
    "                    pass\n",
    "            if (count == 0):\n",
    "                print(\"Not a single fine found...\")\n",
    "                break\n",
    "            l = temp.select('a[href]', limit=count+prevd)\n",
    "            for k in range(0+prevd, len(l)):\n",
    "                print(\"http://xiamen.pbc.gov.cn\" + (l[k]['href']))\n",
    "                w = \"http://xiamen.pbc.gov.cn\" + (l[k]['href'])\n",
    "                linklist.append(w)\n",
    "    \n",
    "    txt = '{y}-{m}-{n}.csv' # put city at the end is better...?\n",
    "    f = open(txt.format(n = name, y = year, m = month), 'w')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['发布日期', '罚单链接'])\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        dlist = []\n",
    "        dlist.append(datelist[i].date().strftime(\"%Y-%m-%d\"))\n",
    "        dlist.append(linklist[i])\n",
    "        print(dlist)\n",
    "        writer.writerow(dlist)\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "### INPUT OFFICIAL WEBSITE INSIDE                           \n",
    "xmSpider('http://xiamen.pbc.gov.cn/xiamen/127703/127721/127728/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Notice (in Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 地区分行（共9家：上海、天津、沈阳、南京、济南、武汉、广州、成都、西安）\n",
    "2. 营业管理部（共2家：北京、**重庆**）\n",
    "3. 省会支行（共20家）\n",
    "4. 非省会支行（共5家：深圳、大连、宁波、青岛、厦门）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Q&A (in Chinese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该爬虫程序是否违法？\n",
    "- 不违法。所有爬取的罚单信息都是公开透明的，不侵犯国家隐私、不危害国家安全。\n",
    "\n",
    "tbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end of the file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
